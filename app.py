import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import string
import joblib
import fitz  # PyMuPDF
import random

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Ä–µ–∑—é–º–µ",
    page_icon="üìÑ",
    layout="wide"
)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    if text is None:
        return ""
    text = re.sub(r'[^\w\s]', ' ', text)  # —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    text = re.sub(r'\d+', ' ', text)      # —É–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä
    text = re.sub(r'\n', ' ', text)       # —É–¥–∞–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
    text = re.sub(r'[a-zA-Z]', ' ', text) # —É–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
    text = re.sub(r'\s+', ' ', text)      # –∑–∞–º–µ–Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –æ–¥–Ω–∏–º
    text = text.lower()                   # –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.translate(str.maketrans('', '', string.punctuation))
    return text

def extract_text_from_pdf(pdf_file):
    text = ""
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        import tempfile
        import os
        
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs('/tmp', exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        temp_path = f"/tmp/{pdf_file.name}"
        with open(temp_path, 'wb') as f:
            f.write(pdf_file.getbuffer())
        
        st.info(f"–§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –ø–æ –ø—É—Ç–∏: {temp_path}")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF –∏–∑ —Ñ–∞–π–ª–∞
        with fitz.open(temp_path) as doc:
            for page in doc:
                text += page.get_text()
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        os.remove(temp_path)
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF: {e}")
        import traceback
        st.code(traceback.format_exc())
        
    return text

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
@st.cache_resource
def load_model():
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
        import joblib
        pipeline = joblib.load('resume_classifier.joblib')
        st.success("–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ —Ñ–∞–π–ª–∞")
        return pipeline
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞
def get_class_specific_comment(class_id):
    # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ —É –Ω–∞—Å –∫–ª–∞—Å—Å—ã 0 –∏ 1, –≥–¥–µ 1 - –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
    if class_id == 1:
        comments = [
            "–ö–∞–Ω–¥–∏–¥–∞—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω. –ü—Ä–æ—Ñ–∏–ª—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø–æ–∑–∏—Ü–∏–∏.",
            "–†–µ–∑—é–º–µ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –Ω–∞–≤—ã–∫–∏ –∏ –æ–ø—ã—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–π –¥–æ–ª–∂–Ω–æ—Å—Ç–∏.",
            "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ. –ö–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–Ω–∏—è–º."
        ]
    else:
        comments = [
            "–ö–∞–Ω–¥–∏–¥–∞—Ç –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º.",
            "–†–µ–∑—é–º–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ–≥–æ –æ–ø—ã—Ç–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏.",
            "–ü—Ä–æ—Ñ–∏–ª—å –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫—Ä–∏—Ç–µ—Ä–∏—è–º –æ—Ç–±–æ—Ä–∞."
        ]
    
    return random.choice(comments)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞
def extract_key_words(text):
    # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã,
    # –Ω–∞–ø—Ä–∏–º–µ—Ä, TF-IDF –∏–ª–∏ KeyBERT
    
    words = text.split()
    # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ
    key_words = set([word for word in words if len(word) > 4])
    # –ë–µ—Ä–µ–º –Ω–µ –±–æ–ª–µ–µ 20 —Å–ª–æ–≤
    return list(key_words)[:20]

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ
def enhance_comment_with_text(base_comment, text, predicted_class):
    # –ï—Å–ª–∏ –±–∞–∑–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø—É—Å—Ç, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
    if not base_comment:
        base_comment = get_class_specific_comment(predicted_class)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–∑—é–º–µ (–¥–æ 1000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
    text_sample = text[:1000].lower() if text else ""
    
    # –ù–µ—Å–∫–æ–ª—å–∫–æ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
    enhancements = []
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã
    experience_indicators = ["–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã", "–ª–µ—Ç –æ–ø—ã—Ç–∞", "–≥–æ–¥ –æ–ø—ã—Ç–∞", "–æ–ø—ã—Ç –≤", "—Ä–∞–±–æ—Ç–∞–ª –≤", "–∑–∞–Ω–∏–º–∞–ª –¥–æ–ª–∂–Ω–æ—Å—Ç—å"]
    for indicator in experience_indicators:
        if indicator in text_sample:
            enhancements.append("–ò–º–µ–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã.")
            break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
    education_indicators = ["–≤—ã—Å—à–µ–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–≤—É–∑", "–±–∞–∫–∞–ª–∞–≤—Ä", "–º–∞–≥–∏—Å—Ç—Ä", "—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å"]
    for indicator in education_indicators:
        if indicator in text_sample:
            enhancements.append("–û–±–ª–∞–¥–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–º –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º.")
            break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏/–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
    skill_indicators = ["–Ω–∞–≤—ã–∫–∏", "–≤–ª–∞–¥–µ–Ω–∏–µ", "–∑–Ω–∞–Ω–∏–µ", "—É–º–µ–Ω–∏–µ", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"]
    for indicator in skill_indicators:
        if indicator in text_sample:
            enhancements.append("–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏.")
            break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏ (–∏–∑ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞ –≤–∏–¥–Ω–æ, —á—Ç–æ —ç—Ç–æ –≤–∞–∂–Ω–æ)
    sales_indicators = ["–ø—Ä–æ–¥–∞–∂", "–∫–ª–∏–µ–Ω—Ç", "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å–¥–µ–ª–∫", "–ø–µ—Ä–µ–≥–æ–≤–æ—Ä", "–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏"]
    for indicator in sales_indicators:
        if indicator in text_sample:
            enhancements.append("–ò–º–µ–µ—Ç –æ–ø—ã—Ç –≤ —Å—Ñ–µ—Ä–µ –ø—Ä–æ–¥–∞–∂.")
            break
    
    # –î–æ–ø–æ–ª–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º, –∏–∑–±–µ–≥–∞—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
    if enhancements:
        random.shuffle(enhancements)
        enhancements = enhancements[:2]  # –ë–µ—Ä–µ–º –Ω–µ –±–æ–ª–µ–µ 2 –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π
        enhanced_comment = base_comment + " " + " ".join(enhancements)
        return enhanced_comment
    
    return base_comment

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
def generate_comment(text, model, prediction):
    try:
        # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞
        base_comment = get_class_specific_comment(prediction)
        
        # –£–ª—É—á—à–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Ä–µ–∑—é–º–µ
        enhanced_comment = enhance_comment_with_text(base_comment, text, prediction)
        
        return enhanced_comment
    except Exception as e:
        st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è: {str(e)}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        if prediction == 1:
            return "–ö–∞–Ω–¥–∏–¥–∞—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω –∫ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é."
        else:
            return "–ö–∞–Ω–¥–∏–¥–∞—Ç –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω –∫ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—é."

# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
def main():
    st.title("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ä–µ–∑—é–º–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    model = load_model()
    
    if model is None:
        st.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª resume_classifier.joblib —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")
        st.stop()
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ PDF —Ñ–∞–π–ª–æ–≤
    st.header("–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ñ–∞–π–ª—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    uploaded_files = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª—ã", type="pdf", accept_multiple_files=True)
    
    if uploaded_files:
        with st.spinner("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã..."):
            results = []
            
            for file in uploaded_files:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
                text = extract_text_from_pdf(file)
                clean_text_content = clean_text(text)
                
                if not clean_text_content:
                    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {file.name}")
                    continue
                
                try:
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    prediction = model.predict([clean_text_content])[0]
                    # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                    proba = model.predict_proba([clean_text_content])[0]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å proba
                    if len(proba) <= prediction:
                        st.warning(f"–û—à–∏–±–∫–∞: –∏–Ω–¥–µ–∫—Å {prediction} –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ {len(proba)}")
                        relevance_prob = 0.5  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                    else:
                        relevance_prob = float(proba[prediction])
                    
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
                    comment = generate_comment(text, model, prediction)
                    
                    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                    results.append({
                        "file": file.name,
                        "predicted_class": int(prediction),
                        "relevance_prob": relevance_prob,
                        "comment": comment
                    })
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file.name}: {str(e)}")
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            if results:
                st.success(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} —Ñ–∞–π–ª–æ–≤")
                df = pd.DataFrame(results)
                st.dataframe(df)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                csv = df.to_csv(index=False)
                st.download_button(
                    label="–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV",
                    data=csv,
                    file_name="classification_results.csv",
                    mime="text/csv"
                )

if __name__ == "__main__":
    main()